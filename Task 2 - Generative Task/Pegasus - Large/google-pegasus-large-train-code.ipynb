{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-28T05:02:28.577712Z","iopub.status.busy":"2023-07-28T05:02:28.576564Z","iopub.status.idle":"2023-07-28T05:04:52.754440Z","shell.execute_reply":"2023-07-28T05:04:52.753014Z","shell.execute_reply.started":"2023-07-28T05:02:28.577638Z"},"trusted":true},"outputs":[],"source":["#for installing all the libraries\n","!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n","!pip install transformers\n","!pip install simpletransformers\n","! pip install -U git+https://github.com/huggingface/transformers.git\n","! pip install -U git+https://github.com/huggingface/accelerate.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:04:52.758978Z","iopub.status.busy":"2023-07-28T05:04:52.758033Z","iopub.status.idle":"2023-07-28T05:04:54.034703Z","shell.execute_reply":"2023-07-28T05:04:54.033330Z","shell.execute_reply.started":"2023-07-28T05:04:52.758931Z"},"trusted":true},"outputs":[],"source":["#for checking gpu details if present any on machine\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:04:54.037587Z","iopub.status.busy":"2023-07-28T05:04:54.037117Z","iopub.status.idle":"2023-07-28T05:05:09.178869Z","shell.execute_reply":"2023-07-28T05:05:09.177397Z","shell.execute_reply.started":"2023-07-28T05:04:54.037546Z"},"trusted":true},"outputs":[],"source":["#for loading evaluate function\n","!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:05:09.183525Z","iopub.status.busy":"2023-07-28T05:05:09.183019Z","iopub.status.idle":"2023-07-28T05:05:29.107697Z","shell.execute_reply":"2023-07-28T05:05:29.106526Z","shell.execute_reply.started":"2023-07-28T05:05:09.183478Z"},"trusted":true},"outputs":[],"source":["#for importing necessary all the libraries\n","import json\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from transformers import pipeline, set_seed\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","import pandas as pd\n","from datasets import load_dataset, load_metric\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","from tqdm import tqdm\n","import torch\n","import nltk\n","import evaluate"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:05:29.110437Z","iopub.status.busy":"2023-07-28T05:05:29.109479Z","iopub.status.idle":"2023-07-28T05:05:29.600316Z","shell.execute_reply":"2023-07-28T05:05:29.599133Z","shell.execute_reply.started":"2023-07-28T05:05:29.110394Z"},"trusted":true},"outputs":[],"source":["# to load .jsonl dataset files in the code and returning dataframe of important columns for generation task.(mainly two columns, content and their respective spoilers)\n","def load_dataset(file_name):\n","    data = []\n","    with open(file_name, encoding='utf8') as f:\n","        for line in f:\n","            example = json.loads(line)\n","            post_text = example['postText'][0]\n","            title = example['targetTitle']\n","            paragraphs = ' '.join(example['targetParagraphs'])\n","            spoiler = example['spoiler'][0]\n","            data.append({'text': post_text + ' - ' + title + paragraphs, 'spoiler': spoiler})\n","    return pd.DataFrame(data)\n","train_data = load_dataset(r'Your train dataset jsonl file location')\n","validation_data = load_dataset(r'Your validation dataset jsonl file location')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:05:29.619371Z","iopub.status.busy":"2023-07-28T05:05:29.617809Z","iopub.status.idle":"2023-07-28T05:05:29.629597Z","shell.execute_reply":"2023-07-28T05:05:29.628198Z","shell.execute_reply.started":"2023-07-28T05:05:29.619318Z"},"trusted":true},"outputs":[],"source":["# converting both dataframes to numpy to further apply preprocessing on it.\n","train_data = np.array(train_data)\n","validation_data = np.array(validation_data)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:05:29.633250Z","iopub.status.busy":"2023-07-28T05:05:29.631763Z","iopub.status.idle":"2023-07-28T05:05:30.101144Z","shell.execute_reply":"2023-07-28T05:05:30.099953Z","shell.execute_reply.started":"2023-07-28T05:05:29.633103Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n","400\n"]}],"source":["# For removing all the extra special characters from the data\n","characters = ['!','\"','#','$','%','&','(',')','*','+','/',':',';','<','=','>','@','^','`','|','~','\\t','[',']','{','}','\\\\','.','-']\n","for i in range(len(train_data)):\n","    for j in characters:\n","        train_data[i][0] = train_data[i][0].replace(j,\"\")\n","        train_data[i][1] = train_data[i][1].replace(j,\"\")\n","\n","for i in range(len(validation_data)):\n","    for j in characters:\n","        validation_data[i][0] = validation_data[i][0].replace(j,\"\")\n","        validation_data[i][1] = validation_data[i][1].replace(j,\"\")\n","\n","        \n","print(len(train_data))\n","print(len(validation_data))\n","\n","train_data = pd.DataFrame(train_data)\n","validation_data = pd.DataFrame(validation_data)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:05:30.104036Z","iopub.status.busy":"2023-07-28T05:05:30.102732Z","iopub.status.idle":"2023-07-28T05:05:30.770944Z","shell.execute_reply":"2023-07-28T05:05:30.769728Z","shell.execute_reply.started":"2023-07-28T05:05:30.103996Z"},"trusted":true},"outputs":[],"source":["# Converting dataframes and storing them as .csv file\n","train_data.to_csv('Your train dataset jsonl file location which will store dataframe .csv format', index=False)\n","validation_data.to_csv('Your validation dataset jsonl file location which will store dataframe .csv format', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:05:30.782679Z","iopub.status.busy":"2023-07-28T05:05:30.779497Z","iopub.status.idle":"2023-07-28T05:05:32.015622Z","shell.execute_reply":"2023-07-28T05:05:32.014538Z","shell.execute_reply.started":"2023-07-28T05:05:30.782604Z"},"trusted":true},"outputs":[],"source":["# loading dataset in the form which is acceptable to pretrained model\n","from datasets import load_dataset\n","\n","dataset_file = 'Your .csv file location from above'\n","dataset_file_1 = 'Your .csv file location from above'\n","train_data = load_dataset('csv', data_files=dataset_file)\n","val_data = load_dataset('csv', data_files=dataset_file_1)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:05:32.018254Z","iopub.status.busy":"2023-07-28T05:05:32.017190Z","iopub.status.idle":"2023-07-28T05:05:32.026280Z","shell.execute_reply":"2023-07-28T05:05:32.025190Z","shell.execute_reply.started":"2023-07-28T05:05:32.018213Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2070\n"]}],"source":["print(train_data['train'][1]['1'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:05:32.037131Z","iopub.status.busy":"2023-07-28T05:05:32.030036Z","iopub.status.idle":"2023-07-28T05:06:22.320036Z","shell.execute_reply":"2023-07-28T05:06:22.318838Z","shell.execute_reply.started":"2023-07-28T05:05:32.037082Z"},"trusted":true},"outputs":[],"source":["# loading pretrained model pegasus-large on gpu of the server along with it's tokenizer\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","model_ckpt = \"google/pegasus-large\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:06:39.788112Z","iopub.status.busy":"2023-07-28T05:06:39.787648Z","iopub.status.idle":"2023-07-28T05:06:50.619448Z","shell.execute_reply":"2023-07-28T05:06:50.618270Z","shell.execute_reply.started":"2023-07-28T05:06:39.788067Z"},"trusted":true},"outputs":[],"source":["# Creating embeddings of the data\n","def convert_examples_to_features(example_batch):\n","    input_encodings = tokenizer(example_batch['0'] , max_length = 256, truncation = True , padding= \"max_length\")\n","    \n","    with tokenizer.as_target_tokenizer():\n","        target_encodings = tokenizer(example_batch['1'], max_length = 50, truncation = True , padding= \"max_length\")\n","        \n","    return {\n","        'input_ids' : input_encodings['input_ids'],\n","        'attention_mask': input_encodings['attention_mask'],\n","        'labels': target_encodings['input_ids']\n","    }\n","    \n","train = train_data.map(convert_examples_to_features, batched = True)\n","val = val_data.map(convert_examples_to_features, batched = True)\n","          "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:06:50.622175Z","iopub.status.busy":"2023-07-28T05:06:50.621396Z","iopub.status.idle":"2023-07-28T05:06:50.632002Z","shell.execute_reply":"2023-07-28T05:06:50.630811Z","shell.execute_reply.started":"2023-07-28T05:06:50.622130Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['0', '1'],\n","        num_rows: 400\n","    })\n","})\n"]}],"source":["print(val_data)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:06:50.634713Z","iopub.status.busy":"2023-07-28T05:06:50.633914Z","iopub.status.idle":"2023-07-28T05:06:50.657236Z","shell.execute_reply":"2023-07-28T05:06:50.655975Z","shell.execute_reply.started":"2023-07-28T05:06:50.634639Z"},"trusted":true},"outputs":[],"source":["#Intializing data collector\n","from transformers import DataCollatorForSeq2Seq\n","\n","seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:06:50.661040Z","iopub.status.busy":"2023-07-28T05:06:50.658874Z","iopub.status.idle":"2023-07-28T05:06:50.717342Z","shell.execute_reply":"2023-07-28T05:06:50.716146Z","shell.execute_reply.started":"2023-07-28T05:06:50.660994Z"},"trusted":true},"outputs":[],"source":["#Defining training arguments\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","trainer_args = Seq2SeqTrainingArguments(\n","    output_dir='location where ypu want to store your model', num_train_epochs=10, warmup_steps=0,\n","    per_device_train_batch_size=2, per_device_eval_batch_size=2,\n","    weight_decay=0.01, logging_steps=10,\n","    evaluation_strategy='epoch', save_steps=1e6,\n","    gradient_accumulation_steps=16\n",") "]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:06:50.721698Z","iopub.status.busy":"2023-07-28T05:06:50.720911Z","iopub.status.idle":"2023-07-28T05:06:50.746765Z","shell.execute_reply":"2023-07-28T05:06:50.745605Z","shell.execute_reply.started":"2023-07-28T05:06:50.721638Z"},"trusted":true},"outputs":[],"source":["# Passing trainer arguments\n","trainer = Seq2SeqTrainer(model=model_pegasus, args=trainer_args,\n","                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n","                  train_dataset=train['train'], \n","                  eval_dataset=val['train'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T05:06:50.749214Z","iopub.status.busy":"2023-07-28T05:06:50.748744Z","iopub.status.idle":"2023-07-28T07:14:32.981725Z","shell.execute_reply":"2023-07-28T07:14:32.980491Z","shell.execute_reply.started":"2023-07-28T05:06:50.749170Z"},"trusted":true},"outputs":[],"source":["# for starting training of the model\n","trainer.train()\n"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T07:24:46.060494Z","iopub.status.busy":"2023-07-28T07:24:46.060045Z","iopub.status.idle":"2023-07-28T07:24:52.815551Z","shell.execute_reply":"2023-07-28T07:24:52.814170Z","shell.execute_reply.started":"2023-07-28T07:24:46.060460Z"},"trusted":true},"outputs":[],"source":["# For saving model after training \n","model_pegasus.save_pretrained(\"location where ypu want to store your model\")"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T07:25:17.584240Z","iopub.status.busy":"2023-07-28T07:25:17.583559Z","iopub.status.idle":"2023-07-28T07:25:17.837172Z","shell.execute_reply":"2023-07-28T07:25:17.835921Z","shell.execute_reply.started":"2023-07-28T07:25:17.584202Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('tokenizer/tokenizer_config.json',\n"," 'tokenizer/special_tokens_map.json',\n"," 'tokenizer/spiece.model',\n"," 'tokenizer/added_tokens.json',\n"," 'tokenizer/tokenizer.json')"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["#For saving it's tokenizer after saving\n","tokenizer.save_pretrained(\"location where ypu want to store your model tokenizer\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-28T08:02:12.365011Z","iopub.status.busy":"2023-07-28T08:02:12.364533Z","iopub.status.idle":"2023-07-28T08:04:22.857021Z","shell.execute_reply":"2023-07-28T08:04:22.855815Z","shell.execute_reply.started":"2023-07-28T08:02:12.364963Z"},"trusted":true},"outputs":[],"source":["#I am using hugging face repository(private) to store my trained model from my training code, and by using hugging face token I am loading model in this inference code.\n","#for installing all the libraries\n","!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n","!pip install transformers\n","!pip install simpletransformers\n","! pip install -U git+https://github.com/huggingface/transformers.git\n","! pip install -U git+https://github.com/huggingface/accelerate.git"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T08:04:22.860373Z","iopub.status.busy":"2023-07-28T08:04:22.859651Z","iopub.status.idle":"2023-07-28T08:04:40.895318Z","shell.execute_reply":"2023-07-28T08:04:40.894193Z","shell.execute_reply.started":"2023-07-28T08:04:22.860332Z"},"trusted":true},"outputs":[],"source":["#for importing necessary all the libraries\n","import json\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from transformers import pipeline, set_seed\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","import pandas as pd\n","from datasets import load_dataset, load_metric\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","from tqdm import tqdm\n","import torch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T08:04:40.897687Z","iopub.status.busy":"2023-07-28T08:04:40.896885Z","iopub.status.idle":"2023-07-28T08:04:41.375125Z","shell.execute_reply":"2023-07-28T08:04:41.373873Z","shell.execute_reply.started":"2023-07-28T08:04:40.897651Z"},"trusted":true},"outputs":[],"source":["# Code for loading, preprocessing and transforming data in the form which is acceptable for pretrained model.\n","def load_dataset(file_name):\n","    data = []\n","    with open(file_name, encoding='utf8') as f:\n","        for line in f:\n","            example = json.loads(line)\n","            post_text = example['postText'][0]\n","            title = example['targetTitle']\n","            paragraphs = ' '.join(example['targetParagraphs'])\n","            id_1 = example['id']\n","            data.append({'text': post_text + ' - ' + title + paragraphs})\n","    return pd.DataFrame(data)\n","test_data_1 = load_dataset(r'Your test dataset jsonl file location')\n","test_data = np.array(test_data_1)\n","characters = ['!','\"','#','$','%','&','(',')','*','+','/',':',';','<','=','>','@','^','`','|','~','\\t','[',']','{','}','\\\\','.','-']\n","print(len(test_data))\n","for i in range(len(test_data)):\n","    for j in characters:\n","        test_data[i][0] = test_data[i][0].replace(j,\"\")\n","test_data = pd.DataFrame(test_data)\n","test_data.to_csv('Your test dataset jsonl file location which will store dataframe .csv format', index=False)\n"," \n","from datasets import load_dataset\n"," \n","dataset_file = 'Your .csv file location from above'\n","test_data = load_dataset('csv', data_files=dataset_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T08:04:41.388124Z","iopub.status.busy":"2023-07-28T08:04:41.387222Z","iopub.status.idle":"2023-07-28T08:04:41.433615Z","shell.execute_reply":"2023-07-28T08:04:41.432591Z","shell.execute_reply.started":"2023-07-28T08:04:41.388088Z"},"trusted":true},"outputs":[],"source":["# for creating dataframe of id's that are used test data\n","def load_dataset(file_name):\n","    data = []\n","    with open(file_name, encoding='utf8') as f:\n","        for line in f:\n","            example = json.loads(line)\n","            id_1 = example['id']\n","            data.append({'id': id_1})\n","    return pd.DataFrame(data)\n","id_data = load_dataset(r'Your test dataset jsonl file location')\n","print(id_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T08:04:41.436054Z","iopub.status.busy":"2023-07-28T08:04:41.434968Z","iopub.status.idle":"2023-07-28T08:04:41.603262Z","shell.execute_reply":"2023-07-28T08:04:41.602225Z","shell.execute_reply.started":"2023-07-28T08:04:41.436022Z"},"trusted":true},"outputs":[],"source":["#hugging face login through token to access the model and tokenizer\n","from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T08:04:41.606864Z","iopub.status.busy":"2023-07-28T08:04:41.606577Z","iopub.status.idle":"2023-07-28T08:05:19.793449Z","shell.execute_reply":"2023-07-28T08:05:19.792384Z","shell.execute_reply.started":"2023-07-28T08:04:41.606839Z"},"trusted":true},"outputs":[],"source":["# loading pretrained model google-pegasus-large which was trained on train dataset (stored in private hugging face repository) on gpu of the server along with it's tokenizer\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","tokenizer = AutoTokenizer.from_pretrained(\"vinamra98/pegasus-clickbait-model\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"vinamra98/pegasus-clickbait-model\").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# creating pipeline to generate output (spolier) of test data and setting parameters for pipeline\n","lst=[]\n","gen_kwargs = {\"length_penalty\": 0, \"num_beams\":8, \"max_length\": 50}\n","pipe = pipeline(\"summarization\", model=model,tokenizer=tokenizer, device=\"cuda\")\n","for i in range(len(test_data['train'])):\n","    sent = pipe(test_data['train'][i]['0'],**gen_kwargs, truncation=True)\n","    lst.append(sent[0]['summary_text']) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#adding spolier section to id's dataframe with all the spoiler generated by the model\n","id_data['spoiler']=lst"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T08:10:44.484265Z","iopub.status.busy":"2023-07-28T08:10:44.483634Z","iopub.status.idle":"2023-07-28T08:10:44.498032Z","shell.execute_reply":"2023-07-28T08:10:44.497012Z","shell.execute_reply.started":"2023-07-28T08:10:44.484226Z"},"trusted":true},"outputs":[],"source":["#code for generating .csv file for final submission to the contest\n","id_data.to_csv('generative_task_pegasus.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

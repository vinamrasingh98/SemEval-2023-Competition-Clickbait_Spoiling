{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-28T17:56:24.921636Z","iopub.status.busy":"2023-07-28T17:56:24.921265Z","iopub.status.idle":"2023-07-28T17:58:33.859844Z","shell.execute_reply":"2023-07-28T17:58:33.858602Z","shell.execute_reply.started":"2023-07-28T17:56:24.921607Z"},"trusted":true},"outputs":[],"source":["#for installing all the libraries\n","!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n","!pip install transformers\n","!pip install simpletransformers\n","! pip install -U git+https://github.com/huggingface/transformers.git\n","! pip install -U git+https://github.com/huggingface/accelerate.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:58:33.863950Z","iopub.status.busy":"2023-07-28T17:58:33.862960Z","iopub.status.idle":"2023-07-28T17:58:34.934595Z","shell.execute_reply":"2023-07-28T17:58:34.933160Z","shell.execute_reply.started":"2023-07-28T17:58:33.863907Z"},"trusted":true},"outputs":[],"source":["#for checking gpu details if present any on machine\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:58:34.939635Z","iopub.status.busy":"2023-07-28T17:58:34.939113Z","iopub.status.idle":"2023-07-28T17:58:47.526661Z","shell.execute_reply":"2023-07-28T17:58:47.525445Z","shell.execute_reply.started":"2023-07-28T17:58:34.939581Z"},"trusted":true},"outputs":[],"source":["#for loading evaluate function\n","!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:58:47.530261Z","iopub.status.busy":"2023-07-28T17:58:47.529752Z","iopub.status.idle":"2023-07-28T17:59:16.339908Z","shell.execute_reply":"2023-07-28T17:59:16.338893Z","shell.execute_reply.started":"2023-07-28T17:58:47.530224Z"},"trusted":true},"outputs":[],"source":["#for importing necessary all the libraries\n","import json\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from transformers import pipeline, set_seed\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","import pandas as pd\n","from datasets import load_dataset, load_metric\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","from tqdm import tqdm\n","import torch\n","import nltk\n","import evaluate"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:59:16.342385Z","iopub.status.busy":"2023-07-28T17:59:16.341276Z","iopub.status.idle":"2023-07-28T17:59:16.798357Z","shell.execute_reply":"2023-07-28T17:59:16.797307Z","shell.execute_reply.started":"2023-07-28T17:59:16.342346Z"},"trusted":true},"outputs":[],"source":["# to load .jsonl dataset files in the code and returning dataframe of important columns for generation task.(mainly two columns, content and their respective spoilers)\n","def load_dataset(file_name):\n","    data = []\n","    with open(file_name, encoding='utf8') as f:\n","        for line in f:\n","            example = json.loads(line)\n","            post_text = example['postText'][0]\n","            title = example['targetTitle']\n","            paragraphs = ' '.join(example['targetParagraphs'])\n","            spoiler = example['spoiler'][0]\n","            data.append({'text': post_text + ' - ' + title + paragraphs, 'spoiler': spoiler})\n","    return pd.DataFrame(data)\n","train_data = load_dataset(r'Your train dataset jsonl file location')\n","validation_data = load_dataset(r'Your validation dataset jsonl file location')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:59:16.812195Z","iopub.status.busy":"2023-07-28T17:59:16.811757Z","iopub.status.idle":"2023-07-28T17:59:16.824529Z","shell.execute_reply":"2023-07-28T17:59:16.823684Z","shell.execute_reply.started":"2023-07-28T17:59:16.812161Z"},"trusted":true},"outputs":[],"source":["# converting both dataframes to numpy to further apply preprocessing on it.\n","train_data = np.array(train_data)\n","validation_data = np.array(validation_data)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:59:16.827837Z","iopub.status.busy":"2023-07-28T17:59:16.827587Z","iopub.status.idle":"2023-07-28T17:59:17.231435Z","shell.execute_reply":"2023-07-28T17:59:17.230455Z","shell.execute_reply.started":"2023-07-28T17:59:16.827815Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n","400\n"]}],"source":["# For removing all the extra special characters from the data\n","characters = ['!','\"','#','$','%','&','(',')','*','+','/',':',';','<','=','>','@','^','`','|','~','\\t','[',']','{','}','\\\\','.','-']\n","for i in range(len(train_data)):\n","    for j in characters:\n","        train_data[i][0] = train_data[i][0].replace(j,\"\")\n","        train_data[i][1] = train_data[i][1].replace(j,\"\")\n","\n","for i in range(len(validation_data)):\n","    for j in characters:\n","        validation_data[i][0] = validation_data[i][0].replace(j,\"\")\n","        validation_data[i][1] = validation_data[i][1].replace(j,\"\")\n","\n","        \n","print(len(train_data))\n","print(len(validation_data))\n","\n","train_data = pd.DataFrame(train_data)\n","validation_data = pd.DataFrame(validation_data)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:59:17.233364Z","iopub.status.busy":"2023-07-28T17:59:17.232988Z","iopub.status.idle":"2023-07-28T17:59:17.744618Z","shell.execute_reply":"2023-07-28T17:59:17.743648Z","shell.execute_reply.started":"2023-07-28T17:59:17.233331Z"},"trusted":true},"outputs":[],"source":["# Converting dataframes and storing them as .csv file\n","train_data.to_csv('Your train dataset jsonl file location which will store dataframe .csv format', index=False)\n","validation_data.to_csv('Your validation dataset jsonl file location which will store dataframe .csv format', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:59:17.749367Z","iopub.status.busy":"2023-07-28T17:59:17.749014Z","iopub.status.idle":"2023-07-28T17:59:18.528328Z","shell.execute_reply":"2023-07-28T17:59:18.527373Z","shell.execute_reply.started":"2023-07-28T17:59:17.749339Z"},"trusted":true},"outputs":[],"source":["# loading dataset in the form which is acceptable to pretrained model\n","from datasets import load_dataset\n","\n","dataset_file = 'Your .csv file location from above'\n","dataset_file_1 = 'Your .csv file location from above'\n","train_data = load_dataset('csv', data_files=dataset_file)\n","val_data = load_dataset('csv', data_files=dataset_file_1)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:59:18.530930Z","iopub.status.busy":"2023-07-28T17:59:18.529927Z","iopub.status.idle":"2023-07-28T17:59:18.536808Z","shell.execute_reply":"2023-07-28T17:59:18.535484Z","shell.execute_reply.started":"2023-07-28T17:59:18.530882Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2070\n"]}],"source":["print(train_data['train'][1]['1'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:59:18.539112Z","iopub.status.busy":"2023-07-28T17:59:18.538675Z","iopub.status.idle":"2023-07-28T17:59:36.085037Z","shell.execute_reply":"2023-07-28T17:59:36.083975Z","shell.execute_reply.started":"2023-07-28T17:59:18.539052Z"},"trusted":true},"outputs":[],"source":["# loading pretrained model t5-base on gpu of the server along with it's tokenizer\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","model_ckpt = \"t5-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T17:59:51.152736Z","iopub.status.busy":"2023-07-28T17:59:51.152293Z","iopub.status.idle":"2023-07-28T18:00:02.273846Z","shell.execute_reply":"2023-07-28T18:00:02.272843Z","shell.execute_reply.started":"2023-07-28T17:59:51.152691Z"},"trusted":true},"outputs":[],"source":["# Creating embeddings of the data\n","def convert_examples_to_features(example_batch):\n","    input_encodings = tokenizer(example_batch['0'] , max_length = 1024, truncation = True , padding= \"max_length\")\n","    \n","    with tokenizer.as_target_tokenizer():\n","        target_encodings = tokenizer(example_batch['1'], max_length = 50, truncation = True , padding= \"max_length\")\n","        \n","    return {\n","        'input_ids' : input_encodings['input_ids'],\n","        'attention_mask': input_encodings['attention_mask'],\n","        'labels': target_encodings['input_ids']\n","    }\n","    \n","train = train_data.map(convert_examples_to_features, batched = True)\n","val = val_data.map(convert_examples_to_features, batched = True)\n","          "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T18:00:02.276272Z","iopub.status.busy":"2023-07-28T18:00:02.275830Z","iopub.status.idle":"2023-07-28T18:00:02.281725Z","shell.execute_reply":"2023-07-28T18:00:02.280815Z","shell.execute_reply.started":"2023-07-28T18:00:02.276231Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['0', '1'],\n","        num_rows: 400\n","    })\n","})\n"]}],"source":["print(val_data)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T18:00:02.284311Z","iopub.status.busy":"2023-07-28T18:00:02.283611Z","iopub.status.idle":"2023-07-28T18:00:02.299336Z","shell.execute_reply":"2023-07-28T18:00:02.298473Z","shell.execute_reply.started":"2023-07-28T18:00:02.284276Z"},"trusted":true},"outputs":[],"source":["#Intializing data collector\n","from transformers import DataCollatorForSeq2Seq\n","\n","seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T18:00:02.301275Z","iopub.status.busy":"2023-07-28T18:00:02.300847Z","iopub.status.idle":"2023-07-28T18:00:02.335870Z","shell.execute_reply":"2023-07-28T18:00:02.335054Z","shell.execute_reply.started":"2023-07-28T18:00:02.301240Z"},"trusted":true},"outputs":[],"source":["#Defining training arguments\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","trainer_args = Seq2SeqTrainingArguments(\n","    output_dir='location where ypu want to store your model', num_train_epochs=10, warmup_steps=0,\n","    per_device_train_batch_size=2, per_device_eval_batch_size=2,\n","    weight_decay=0.01, logging_steps=10,\n","    evaluation_strategy='epoch', save_steps=1e6,\n","    gradient_accumulation_steps=16\n",") "]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T18:00:02.338637Z","iopub.status.busy":"2023-07-28T18:00:02.337958Z","iopub.status.idle":"2023-07-28T18:00:02.357365Z","shell.execute_reply":"2023-07-28T18:00:02.356453Z","shell.execute_reply.started":"2023-07-28T18:00:02.338604Z"},"trusted":true},"outputs":[],"source":["# Passing trainer arguments\n","trainer = Seq2SeqTrainer(model=model_pegasus, args=trainer_args,\n","                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n","                  train_dataset=train['train'], \n","                  eval_dataset=val['train'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T18:00:02.359357Z","iopub.status.busy":"2023-07-28T18:00:02.358988Z","iopub.status.idle":"2023-07-28T19:59:01.723965Z","shell.execute_reply":"2023-07-28T19:59:01.723016Z","shell.execute_reply.started":"2023-07-28T18:00:02.359324Z"},"trusted":true},"outputs":[],"source":["# for starting training of the model\n","trainer.train()\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:02:55.312876Z","iopub.status.busy":"2023-07-28T20:02:55.312508Z","iopub.status.idle":"2023-07-28T20:02:57.306521Z","shell.execute_reply":"2023-07-28T20:02:57.305353Z","shell.execute_reply.started":"2023-07-28T20:02:55.312846Z"},"trusted":true},"outputs":[],"source":["# For saving model after training\n","model_pegasus.save_pretrained(\"location where ypu want to store your model\")"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:02:57.309117Z","iopub.status.busy":"2023-07-28T20:02:57.308493Z","iopub.status.idle":"2023-07-28T20:02:57.390171Z","shell.execute_reply":"2023-07-28T20:02:57.389134Z","shell.execute_reply.started":"2023-07-28T20:02:57.309057Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('tokenizer/tokenizer_config.json',\n"," 'tokenizer/special_tokens_map.json',\n"," 'tokenizer/spiece.model',\n"," 'tokenizer/added_tokens.json',\n"," 'tokenizer/tokenizer.json')"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["#For saving it's tokenizer after saving\n","tokenizer.save_pretrained(\"location where ypu want to store your model tokenizer\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-28T20:24:33.103614Z","iopub.status.busy":"2023-07-28T20:24:33.103161Z","iopub.status.idle":"2023-07-28T20:26:34.815870Z","shell.execute_reply":"2023-07-28T20:26:34.814658Z","shell.execute_reply.started":"2023-07-28T20:24:33.103579Z"},"trusted":true},"outputs":[],"source":["#I am using hugging face repository(private) to store my trained model from my training code, and by using hugging face token I am loading model in this inference code.\n","#for installing all the libraries\n","!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n","!pip install transformers\n","!pip install simpletransformers\n","! pip install -U git+https://github.com/huggingface/transformers.git\n","! pip install -U git+https://github.com/huggingface/accelerate.git"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:26:34.820257Z","iopub.status.busy":"2023-07-28T20:26:34.819543Z","iopub.status.idle":"2023-07-28T20:26:51.416011Z","shell.execute_reply":"2023-07-28T20:26:51.415067Z","shell.execute_reply.started":"2023-07-28T20:26:34.820211Z"},"trusted":true},"outputs":[],"source":["#for importing necessary all the libraries\n","import json\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from transformers import pipeline, set_seed\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","import pandas as pd\n","from datasets import load_dataset, load_metric\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","from tqdm import tqdm\n","import torch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:26:51.418433Z","iopub.status.busy":"2023-07-28T20:26:51.417505Z","iopub.status.idle":"2023-07-28T20:26:51.869128Z","shell.execute_reply":"2023-07-28T20:26:51.868199Z","shell.execute_reply.started":"2023-07-28T20:26:51.418398Z"},"trusted":true},"outputs":[],"source":["# Code for loading, preprocessing and transforming data in the form which is acceptable for pretrained model.\n","def load_dataset(file_name):\n","    data = []\n","    with open(file_name, encoding='utf8') as f:\n","        for line in f:\n","            example = json.loads(line)\n","            post_text = example['postText'][0]\n","            title = example['targetTitle']\n","            paragraphs = ' '.join(example['targetParagraphs'])\n","            id_1 = example['id']\n","            data.append({'text': post_text + ' - ' + title + paragraphs})\n","    return pd.DataFrame(data)\n","test_data_1 = load_dataset(r'Your test dataset jsonl file location')\n","test_data = np.array(test_data_1)\n","characters = ['!','\"','#','$','%','&','(',')','*','+','/',':',';','<','=','>','@','^','`','|','~','\\t','[',']','{','}','\\\\','.','-']\n","print(len(test_data))\n","for i in range(len(test_data)):\n","    for j in characters:\n","        test_data[i][0] = test_data[i][0].replace(j,\"\")\n","test_data = pd.DataFrame(test_data)\n","test_data.to_csv('Your test dataset jsonl file location which will store dataframe .csv format', index=False)\n"," \n","from datasets import load_dataset\n"," \n","dataset_file = 'Your .csv file location from above'\n","test_data = load_dataset('csv', data_files=dataset_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:26:51.872438Z","iopub.status.busy":"2023-07-28T20:26:51.871516Z","iopub.status.idle":"2023-07-28T20:26:51.910783Z","shell.execute_reply":"2023-07-28T20:26:51.909730Z","shell.execute_reply.started":"2023-07-28T20:26:51.872403Z"},"trusted":true},"outputs":[],"source":["# for creating dataframe of id's that are used test data\n","def load_dataset(file_name):\n","    data = []\n","    with open(file_name, encoding='utf8') as f:\n","        for line in f:\n","            example = json.loads(line)\n","            id_1 = example['id']\n","            data.append({'id': id_1})\n","    return pd.DataFrame(data)\n","id_data = load_dataset(r'Your test dataset jsonl file location')\n","print(id_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:26:51.912468Z","iopub.status.busy":"2023-07-28T20:26:51.912119Z","iopub.status.idle":"2023-07-28T20:26:52.038908Z","shell.execute_reply":"2023-07-28T20:26:52.037773Z","shell.execute_reply.started":"2023-07-28T20:26:51.912436Z"},"trusted":true},"outputs":[],"source":["#hugging face login through token to access the model and tokenizer\n","from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:27:24.284421Z","iopub.status.busy":"2023-07-28T20:27:24.283377Z","iopub.status.idle":"2023-07-28T20:27:48.695398Z","shell.execute_reply":"2023-07-28T20:27:48.694370Z","shell.execute_reply.started":"2023-07-28T20:27:24.284377Z"},"trusted":true},"outputs":[],"source":["# loading pretrained model t5-base which was trained on train dataset (stored in private hugging face repository) on gpu of the server along with it's tokenizer\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","tokenizer = AutoTokenizer.from_pretrained(\"vinamra98/t5-clickbait-model\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"vinamra98/t5-clickbait-model\").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:28:46.960963Z","iopub.status.busy":"2023-07-28T20:28:46.960506Z","iopub.status.idle":"2023-07-28T20:35:02.791926Z","shell.execute_reply":"2023-07-28T20:35:02.790869Z","shell.execute_reply.started":"2023-07-28T20:28:46.960922Z"},"trusted":true},"outputs":[],"source":["# creating pipeline to generate output (spolier) of test data and setting parameters for pipeline\n","lst=[]\n","gen_kwargs = {\"length_penalty\": 0.5, \"num_beams\":8, \"max_length\": 50}\n","pipe = pipeline(\"summarization\", model=model,tokenizer=tokenizer, device=\"cuda\")\n","for i in range(len(test_data['train'])):\n","    sent = pipe(test_data['train'][i]['0'],**gen_kwargs, truncation=True)\n","    lst.append(sent[0]['summary_text']) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:36:02.599666Z","iopub.status.busy":"2023-07-28T20:36:02.599165Z","iopub.status.idle":"2023-07-28T20:36:02.614171Z","shell.execute_reply":"2023-07-28T20:36:02.613213Z","shell.execute_reply.started":"2023-07-28T20:36:02.599628Z"},"trusted":true},"outputs":[],"source":["#adding spolier section to id's dataframe with all the spoiler generated by the model\n","id_data['spoiler']=lst"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:35:26.476819Z","iopub.status.busy":"2023-07-28T20:35:26.476452Z","iopub.status.idle":"2023-07-28T20:35:26.491358Z","shell.execute_reply":"2023-07-28T20:35:26.490368Z","shell.execute_reply.started":"2023-07-28T20:35:26.476789Z"},"trusted":true},"outputs":[],"source":["#code for generating .csv file for final submission to the contest\n","id_data.to_csv('generative_task_t5.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

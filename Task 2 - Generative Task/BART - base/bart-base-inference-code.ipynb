{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-28T22:32:35.504458Z","iopub.status.busy":"2023-07-28T22:32:35.504096Z","iopub.status.idle":"2023-07-28T22:34:35.455064Z","shell.execute_reply":"2023-07-28T22:34:35.453570Z","shell.execute_reply.started":"2023-07-28T22:32:35.504430Z"},"trusted":true},"outputs":[],"source":["#I am using hugging face repository(private) to store my trained model from my training code, and by using hugging face token I am loading model in this inference code.\n","#for installing all the libraries\n","!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n","!pip install transformers\n","!pip install simpletransformers\n","! pip install -U git+https://github.com/huggingface/transformers.git\n","! pip install -U git+https://github.com/huggingface/accelerate.git"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:34:35.458509Z","iopub.status.busy":"2023-07-28T22:34:35.458064Z","iopub.status.idle":"2023-07-28T22:34:52.360792Z","shell.execute_reply":"2023-07-28T22:34:52.359820Z","shell.execute_reply.started":"2023-07-28T22:34:35.458462Z"},"trusted":true},"outputs":[],"source":["#for importing necessary all the libraries\n","import json\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from transformers import pipeline, set_seed\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","import pandas as pd\n","from datasets import load_dataset, load_metric\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","from tqdm import tqdm\n","import torch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:34:52.363129Z","iopub.status.busy":"2023-07-28T22:34:52.362294Z","iopub.status.idle":"2023-07-28T22:34:52.817807Z","shell.execute_reply":"2023-07-28T22:34:52.816700Z","shell.execute_reply.started":"2023-07-28T22:34:52.363090Z"},"trusted":true},"outputs":[],"source":["# Code for loading, preprocessing and transforming data in the form which is acceptable for pretrained model.\n","def load_dataset(file_name):\n","    data = []\n","    with open(file_name, encoding='utf8') as f:\n","        for line in f:\n","            example = json.loads(line)\n","            post_text = example['postText'][0]\n","            title = example['targetTitle']\n","            paragraphs = ' '.join(example['targetParagraphs'])\n","            id_1 = example['id']\n","            data.append({'text': post_text + ' - ' + title + paragraphs})\n","    return pd.DataFrame(data)\n","test_data_1 = load_dataset(r'Your test dataset jsonl file location')\n","test_data = np.array(test_data_1)\n","characters = ['!','\"','#','$','%','&','(',')','*','+','/',':',';','<','=','>','@','^','`','|','~','\\t','[',']','{','}','\\\\','.','-']\n","print(len(test_data))\n","for i in range(len(test_data)):\n","    for j in characters:\n","        test_data[i][0] = test_data[i][0].replace(j,\"\")\n","test_data = pd.DataFrame(test_data)\n","test_data.to_csv('Your test dataset jsonl file location which will store dataframe .csv format', index=False)\n"," \n","from datasets import load_dataset\n"," \n","dataset_file = 'Your .csv file location from above'\n","test_data = load_dataset('csv', data_files=dataset_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:34:52.820322Z","iopub.status.busy":"2023-07-28T22:34:52.819626Z","iopub.status.idle":"2023-07-28T22:34:52.858540Z","shell.execute_reply":"2023-07-28T22:34:52.857651Z","shell.execute_reply.started":"2023-07-28T22:34:52.820286Z"},"trusted":true},"outputs":[],"source":["# for creating dataframe of id's that are used test data\n","def load_dataset(file_name):\n","    data = []\n","    with open(file_name, encoding='utf8') as f:\n","        for line in f:\n","            example = json.loads(line)\n","            id_1 = example['id']\n","            data.append({'id': id_1})\n","    return pd.DataFrame(data)\n","id_data = load_dataset(r'Your test dataset jsonl file location')\n","print(id_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:34:52.861856Z","iopub.status.busy":"2023-07-28T22:34:52.861253Z","iopub.status.idle":"2023-07-28T22:34:52.987192Z","shell.execute_reply":"2023-07-28T22:34:52.986255Z","shell.execute_reply.started":"2023-07-28T22:34:52.861823Z"},"trusted":true},"outputs":[],"source":["#hugging face login through token to access the model and tokenizer\n","from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:34:52.989201Z","iopub.status.busy":"2023-07-28T22:34:52.988588Z","iopub.status.idle":"2023-07-28T22:35:05.711845Z","shell.execute_reply":"2023-07-28T22:35:05.710813Z","shell.execute_reply.started":"2023-07-28T22:34:52.989167Z"},"trusted":true},"outputs":[],"source":["# loading pretrained model bart-base which was trained on train dataset (stored in private hugging face repository) on gpu of the server along with it's tokenizer\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","tokenizer = AutoTokenizer.from_pretrained(\"vinamra98/bart-clickbait-model\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"vinamra98/bart-clickbait-model\").to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:35:05.713793Z","iopub.status.busy":"2023-07-28T22:35:05.713331Z","iopub.status.idle":"2023-07-28T22:37:26.581978Z","shell.execute_reply":"2023-07-28T22:37:26.580943Z","shell.execute_reply.started":"2023-07-28T22:35:05.713758Z"},"trusted":true},"outputs":[],"source":["# creating pipeline to generate output (spolier) of test data and setting parameters for pipeline\n","lst=[]\n","gen_kwargs = {\"length_penalty\": 0, \"num_beams\":8, \"max_length\": 50}\n","pipe = pipeline(\"summarization\", model=model,tokenizer=tokenizer, device=\"cuda\")\n","for i in range(len(test_data['train'])):\n","    sent = pipe(test_data['train'][i]['0'],**gen_kwargs, truncation=True)\n","    lst.append(sent[0]['summary_text']) "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:37:26.583911Z","iopub.status.busy":"2023-07-28T22:37:26.583473Z","iopub.status.idle":"2023-07-28T22:37:26.590361Z","shell.execute_reply":"2023-07-28T22:37:26.589150Z","shell.execute_reply.started":"2023-07-28T22:37:26.583857Z"},"trusted":true},"outputs":[],"source":["#adding spolier section to id's dataframe with all the spoiler generated by the model\n","id_data['spoiler']=lst"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:37:26.592227Z","iopub.status.busy":"2023-07-28T22:37:26.591887Z","iopub.status.idle":"2023-07-28T22:37:26.608684Z","shell.execute_reply":"2023-07-28T22:37:26.607810Z","shell.execute_reply.started":"2023-07-28T22:37:26.592195Z"},"trusted":true},"outputs":[],"source":["#code for generating .csv file for final submission to the contest\n","id_data.to_csv('generative_task_bart(30).csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

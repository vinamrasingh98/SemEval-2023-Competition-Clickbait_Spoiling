{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-28T20:53:57.291558Z","iopub.status.busy":"2023-07-28T20:53:57.291136Z","iopub.status.idle":"2023-07-28T20:56:01.277620Z","shell.execute_reply":"2023-07-28T20:56:01.276187Z","shell.execute_reply.started":"2023-07-28T20:53:57.291517Z"},"trusted":true},"outputs":[],"source":["#for installing all the libraries\n","!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n","!pip install transformers\n","!pip install simpletransformers\n","! pip install -U git+https://github.com/huggingface/transformers.git\n","! pip install -U git+https://github.com/huggingface/accelerate.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:01.281946Z","iopub.status.busy":"2023-07-28T20:56:01.281593Z","iopub.status.idle":"2023-07-28T20:56:02.356919Z","shell.execute_reply":"2023-07-28T20:56:02.355598Z","shell.execute_reply.started":"2023-07-28T20:56:01.281904Z"},"trusted":true},"outputs":[],"source":["#for checking gpu details if present any on machine\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:02.361001Z","iopub.status.busy":"2023-07-28T20:56:02.360548Z","iopub.status.idle":"2023-07-28T20:56:14.933640Z","shell.execute_reply":"2023-07-28T20:56:14.932467Z","shell.execute_reply.started":"2023-07-28T20:56:02.360963Z"},"trusted":true},"outputs":[],"source":["#for loading evaluate function\n","!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:14.938112Z","iopub.status.busy":"2023-07-28T20:56:14.937771Z","iopub.status.idle":"2023-07-28T20:56:33.665554Z","shell.execute_reply":"2023-07-28T20:56:33.664567Z","shell.execute_reply.started":"2023-07-28T20:56:14.938083Z"},"trusted":true},"outputs":[],"source":["#for importing necessary all the libraries\n","import json\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from transformers import pipeline, set_seed\n","import matplotlib.pyplot as plt\n","from datasets import load_dataset\n","import pandas as pd\n","from datasets import load_dataset, load_metric\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","from tqdm import tqdm\n","import torch\n","import nltk\n","import evaluate"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:33.673346Z","iopub.status.busy":"2023-07-28T20:56:33.670166Z","iopub.status.idle":"2023-07-28T20:56:34.141750Z","shell.execute_reply":"2023-07-28T20:56:34.140710Z","shell.execute_reply.started":"2023-07-28T20:56:33.673298Z"},"trusted":true},"outputs":[],"source":["# to load .jsonl dataset files in the code and returning dataframe of important columns for generation task.(mainly two columns, content and their respective spoilers)\n","\n","def load_dataset(file_name):\n","    data = []\n","    with open(file_name, encoding='utf8') as f:\n","        for line in f:\n","            example = json.loads(line)\n","            post_text = example['postText'][0]\n","            title = example['targetTitle']\n","            paragraphs = ' '.join(example['targetParagraphs'])\n","            spoiler = example['spoiler'][0]\n","            data.append({'text': post_text + ' - ' + title + paragraphs, 'spoiler': spoiler})\n","    return pd.DataFrame(data)\n","train_data = load_dataset(r'Your train dataset jsonl file location')\n","validation_data = load_dataset(r'Your validation dataset jsonl file location')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:34.153804Z","iopub.status.busy":"2023-07-28T20:56:34.153012Z","iopub.status.idle":"2023-07-28T20:56:34.168544Z","shell.execute_reply":"2023-07-28T20:56:34.167608Z","shell.execute_reply.started":"2023-07-28T20:56:34.153772Z"},"trusted":true},"outputs":[],"source":["# converting both dataframes to numpy to further apply preprocessing on it.\n","train_data = np.array(train_data)\n","validation_data = np.array(validation_data)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:34.170638Z","iopub.status.busy":"2023-07-28T20:56:34.170153Z","iopub.status.idle":"2023-07-28T20:56:34.577515Z","shell.execute_reply":"2023-07-28T20:56:34.576565Z","shell.execute_reply.started":"2023-07-28T20:56:34.170604Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3200\n","400\n"]}],"source":["# For removing all the extra special characters from the data\n","characters = ['!','\"','#','$','%','&','(',')','*','+','/',':',';','<','=','>','@','^','`','|','~','\\t','[',']','{','}','\\\\','.','-']\n","for i in range(len(train_data)):\n","    for j in characters:\n","        train_data[i][0] = train_data[i][0].replace(j,\"\")\n","        train_data[i][1] = train_data[i][1].replace(j,\"\")\n","\n","for i in range(len(validation_data)):\n","    for j in characters:\n","        validation_data[i][0] = validation_data[i][0].replace(j,\"\")\n","        validation_data[i][1] = validation_data[i][1].replace(j,\"\")\n","\n","        \n","print(len(train_data))\n","print(len(validation_data))\n","\n","train_data = pd.DataFrame(train_data)\n","validation_data = pd.DataFrame(validation_data)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:34.581644Z","iopub.status.busy":"2023-07-28T20:56:34.578785Z","iopub.status.idle":"2023-07-28T20:56:35.109557Z","shell.execute_reply":"2023-07-28T20:56:35.108550Z","shell.execute_reply.started":"2023-07-28T20:56:34.581617Z"},"trusted":true},"outputs":[],"source":["# Converting dataframes and storing them as .csv file\n","train_data.to_csv('Your train dataset jsonl file location which will store dataframe .csv format', index=False)\n","validation_data.to_csv('Your validation dataset jsonl file location which will store dataframe .csv format', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:35.114061Z","iopub.status.busy":"2023-07-28T20:56:35.113733Z","iopub.status.idle":"2023-07-28T20:56:36.340861Z","shell.execute_reply":"2023-07-28T20:56:36.339831Z","shell.execute_reply.started":"2023-07-28T20:56:35.114035Z"},"trusted":true},"outputs":[],"source":["# loading dataset in the form which is acceptable to pretrained model\n","from datasets import load_dataset\n","\n","dataset_file = 'Your .csv file location from above'\n","dataset_file_1 = 'Your .csv file location from above'\n","train_data = load_dataset('csv', data_files=dataset_file)\n","val_data = load_dataset('csv', data_files=dataset_file_1)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:36.343125Z","iopub.status.busy":"2023-07-28T20:56:36.342722Z","iopub.status.idle":"2023-07-28T20:56:36.349436Z","shell.execute_reply":"2023-07-28T20:56:36.348478Z","shell.execute_reply.started":"2023-07-28T20:56:36.343088Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2070\n"]}],"source":["print(train_data['train'][1]['1'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:36.352261Z","iopub.status.busy":"2023-07-28T20:56:36.351323Z","iopub.status.idle":"2023-07-28T20:56:49.332670Z","shell.execute_reply":"2023-07-28T20:56:49.331327Z","shell.execute_reply.started":"2023-07-28T20:56:36.352224Z"},"trusted":true},"outputs":[],"source":["# loading pretrained model bart-base on gpu of the server along with it's tokenizer\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","model_ckpt = \"facebook/bart-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:56:49.334808Z","iopub.status.busy":"2023-07-28T20:56:49.334392Z","iopub.status.idle":"2023-07-28T20:57:00.661488Z","shell.execute_reply":"2023-07-28T20:57:00.660516Z","shell.execute_reply.started":"2023-07-28T20:56:49.334753Z"},"trusted":true},"outputs":[],"source":["# Creating embeddings of the data\n","def convert_examples_to_features(example_batch):\n","    input_encodings = tokenizer(example_batch['0'] , max_length = 1024, truncation = True , padding= \"max_length\")\n","    \n","    with tokenizer.as_target_tokenizer():\n","        target_encodings = tokenizer(example_batch['1'], max_length = 50, truncation = True , padding= \"max_length\")\n","        \n","    return {\n","        'input_ids' : input_encodings['input_ids'],\n","        'attention_mask': input_encodings['attention_mask'],\n","        'labels': target_encodings['input_ids']\n","    }\n","    \n","train = train_data.map(convert_examples_to_features, batched = True)\n","val = val_data.map(convert_examples_to_features, batched = True)\n","          "]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:57:00.663448Z","iopub.status.busy":"2023-07-28T20:57:00.663001Z","iopub.status.idle":"2023-07-28T20:57:00.669659Z","shell.execute_reply":"2023-07-28T20:57:00.668587Z","shell.execute_reply.started":"2023-07-28T20:57:00.663413Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['0', '1'],\n","        num_rows: 400\n","    })\n","})\n"]}],"source":["print(val_data)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:57:00.671459Z","iopub.status.busy":"2023-07-28T20:57:00.671101Z","iopub.status.idle":"2023-07-28T20:57:00.682668Z","shell.execute_reply":"2023-07-28T20:57:00.681761Z","shell.execute_reply.started":"2023-07-28T20:57:00.671426Z"},"trusted":true},"outputs":[],"source":["#Intializing data collector\n","from transformers import DataCollatorForSeq2Seq\n","\n","seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:57:00.684738Z","iopub.status.busy":"2023-07-28T20:57:00.684324Z","iopub.status.idle":"2023-07-28T20:57:00.717430Z","shell.execute_reply":"2023-07-28T20:57:00.716588Z","shell.execute_reply.started":"2023-07-28T20:57:00.684702Z"},"trusted":true},"outputs":[],"source":["#Defining training arguments\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","trainer_args = Seq2SeqTrainingArguments(\n","    output_dir='location where ypu want to store your model', num_train_epochs=10, warmup_steps=0,\n","    per_device_train_batch_size=2, per_device_eval_batch_size=2,\n","    weight_decay=0.01, logging_steps=10,\n","    evaluation_strategy='epoch', save_steps=1e6,\n","    gradient_accumulation_steps=16\n",") "]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:57:00.720092Z","iopub.status.busy":"2023-07-28T20:57:00.719547Z","iopub.status.idle":"2023-07-28T20:57:00.734805Z","shell.execute_reply":"2023-07-28T20:57:00.733908Z","shell.execute_reply.started":"2023-07-28T20:57:00.720059Z"},"trusted":true},"outputs":[],"source":["# Passing trainer arguments\n","trainer = Seq2SeqTrainer(model=model_pegasus, args=trainer_args,\n","                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n","                  train_dataset=train['train'], \n","                  eval_dataset=val['train'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T20:57:00.736751Z","iopub.status.busy":"2023-07-28T20:57:00.736421Z","iopub.status.idle":"2023-07-28T21:57:50.507874Z","shell.execute_reply":"2023-07-28T21:57:50.506895Z","shell.execute_reply.started":"2023-07-28T20:57:00.736720Z"},"trusted":true},"outputs":[],"source":["# for starting training of the model\n","trainer.train()\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:01:48.401359Z","iopub.status.busy":"2023-07-28T22:01:48.399805Z","iopub.status.idle":"2023-07-28T22:01:49.234278Z","shell.execute_reply":"2023-07-28T22:01:49.229907Z","shell.execute_reply.started":"2023-07-28T22:01:48.401312Z"},"trusted":true},"outputs":[],"source":["# For saving model after training\n","model_pegasus.save_pretrained(\"location where ypu want to store your model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-28T22:01:49.240493Z","iopub.status.busy":"2023-07-28T22:01:49.238512Z","iopub.status.idle":"2023-07-28T22:01:49.387211Z","shell.execute_reply":"2023-07-28T22:01:49.386235Z","shell.execute_reply.started":"2023-07-28T22:01:49.240453Z"},"trusted":true},"outputs":[],"source":["#For saving it's tokenizer after saving\n","tokenizer.save_pretrained(\"location where ypu want to store your model tokenizer\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
